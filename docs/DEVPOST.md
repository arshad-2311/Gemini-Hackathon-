# SignBridge - Devpost Submission

## ğŸ’¡ Inspiration

Over **70 million deaf people** worldwide face daily communication barriers. Only 2% have access to qualified sign language interpreters. Existing translation tools are basic word-to-sign dictionaries that ignore grammar, context, and the **spatial nature** of sign language.

We asked: *What if AI could understand not just words, but the entire conversation contextâ€”including the physical environment?*

## ğŸ”§ What It Does

**SignBridge** is an AI-powered sign language translator that:

1. **Translates speech to sign language** in real-time using a 3D animated avatar
2. **Teaches sign language** with AI-powered accuracy analysis and personalized corrections
3. **Points to real objects** detected in your environment (world-first spatial awareness)
4. **Supports multiple dialects** (ASL, BSL, ISL) with proper grammatical adaptation
5. **Generates custom lessons** from any document you upload

## ğŸ—ï¸ How We Built It

### Gemini 2.0 Flash Integration

| Feature | Gemini Capability Used |
|---------|----------------------|
| Translation | Text understanding + ASL grammar generation |
| Teaching | Multimodal analysis of pose data |
| Spatial | Vision API for object detection |
| Dialects | Linguistic reasoning for grammar conversion |
| Lessons | Content generation from documents |

### Tech Stack
- **Frontend**: React + Vite + Three.js (3D avatar)
- **Backend**: Node.js + Express + Socket.IO
- **AI**: Google Gemini 2.0 Flash API
- **Speech**: Web Speech API
- **Real-time**: WebSockets

## ğŸš§ Challenges We Ran Into

1. **Sign language grammar is complex** - ASL uses Topic-Comment structure, not English word order. We engineered prompts to make Gemini understand linguistic nuances.

2. **Real-time performance** - 3D avatar animations at 60fps while processing AI responses required careful optimization (object pooling, throttling, lazy loading).

3. **Spatial integration** - Mapping detected objects to avatar pointing gestures required coordinate transformation between camera space and avatar space.

4. **Cultural accuracy** - Each dialect has cultural expressions that don't translate directly. We added cultural context notes to prevent misunderstandings.

## ğŸ† Accomplishments We're Proud Of

- âœ… **First sign language tool with spatial awareness** - Avatar references real objects
- âœ… **AI teaching agent** - Analyzes signing accuracy and provides specific corrections
- âœ… **3 dialect support** with proper grammatical adaptation
- âœ… **Complete in 7 days** - From concept to working demo
- âœ… **60fps performance** - Smooth animations with AI intelligence
- âœ… **Offline demo mode** - Works without API for reliable presentations

## ğŸ“š What We Learned

- **Sign languages are rich, complex systems** - Not just "signed English"
- **Gemini's multimodal capabilities** enable genuinely new interaction patterns
- **Real-time AI** requires careful architecture (WebSockets, caching, fallbacks)
- **Accessibility tech** has immense potential when combined with modern AI

## ğŸ”® What's Next

| Timeline | Feature |
|----------|---------|
| Q2 2024 | Mobile apps (iOS/Android) |
| Q3 2024 | More dialects (JSL, Auslan, LSF) |
| Q4 2024 | Video call integration (Zoom, Meet) |
| 2025 | Fine-tuned Gemini model for sign language |

## ğŸ¯ Categories

- âœ… **Best Multimodal Application** - Vision + Text + Generation
- âœ… **Best AI Agent** - Teaching agent with memory and feedback
- âœ… **Best Use of Gemini 2.0** - Real-time intelligent responses
- âœ… **Grand Prize** - Novel accessibility solution

## ğŸ”— Links

- **Demo Video**: [YouTube](#)
- **Live Demo**: [signbridge.app](#)
- **GitHub**: [github.com/username/signbridge](#)

## ğŸ‘¥ Team

Built with â¤ï¸ for the **Gemini 3 Hackathon**

---

## Built With

`gemini-api` `react` `nodejs` `threejs` `socket-io` `web-speech-api` `vite` `javascript`
